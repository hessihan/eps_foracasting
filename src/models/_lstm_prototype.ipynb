{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## まずは`torch.nn.LSTM`の挙動チェック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "seq_len = 100 # レコード期間\n",
    "input_dim = 4 # 特徴量数\n",
    "hidden_dim = 10 # hidden unit数\n",
    "num_layers = 1 # rnn hidden layer数\n",
    "batch_size = 1 # ミニバッチしたくないからバッチサイズ=1でいいってことだよね?(塊でやらない)\n",
    "\n",
    "lstm = torch.nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(4, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 全ウェイト数 W_x(i,f,g,o), b_x(i,f,g,o), W_h(i,f,g,o), b_h(i,f,g,o)\n",
    "len(lstm.all_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 4])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W_x(i,f,g,o)\n",
    "# (W_xi|W_xf|W_xg|W_xo) of shape (4*hidden_size, input_size)\n",
    "lstm.weight_ih_l0.size() \n",
    "# lstm.weight_ih_l0 == lstm.all_weights[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W_xi, shape (hidden_size, input_size)\n",
    "lstm.weight_ih_l0[:hidden_dim, :].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b_x(i,f,g,o) \n",
    "# (b_xi|b_xf|b_xg|b_xo), of shape (4*hidden_size)\n",
    "lstm.bias_ih_l0.size()\n",
    "# lstm.bias_ih_l0 == lstm.all_weights[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 10])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# W_h(i,f,g,o)\n",
    "# (W_hi|W_hf|W_hg|W_ho) of shape (4*hidden_size, input_size)\n",
    "lstm.weight_hh_l0.size() \n",
    "# lstm.weight_hh_l0 == lstm.all_weights[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b_h(i,f,g,o) \n",
    "# (b_hi|b_hf|b_hg|b_ho), of shape (4*hidden_size)\n",
    "lstm.bias_hh_l0.size()\n",
    "# lstm.bias_hh_l0 == lstm.all_weights[0][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toy data で`nn.LSTM`のoutputを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy data: input of shape (seq_len, batch, input_size)\n",
    "x = torch.randn(seq_len, batch_size, input_dim)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hidden state (lstm層のアクティベーション) of shape (num_layers, batch, hidden_size):\n",
    "h_0 = torch.randn(num_layers, x.size(1), hidden_dim)\n",
    "h_0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cell state (メモリセル) of shape (num_layers, batch, hidden_size): \n",
    "c_0 = torch.randn(num_layers, x.size(1), hidden_dim)\n",
    "c_0.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run forward pass of LSTM layer\n",
    "output, (h_n, c_n) = lstm(x, (h_0, c_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1, 10])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change shape of h_out inorder to input in linear layer (squeeze batch dim)\n",
    "h_out = output.view(-1, hidden_dim)\n",
    "h_out.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_n.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_n.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True, True, True, True, True, True]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the last row of output equals to h_n\n",
    "output[-1] == h_n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Class オブジェクトを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    long short-term memory network.\n",
    "    Nso batches (batch_size=1), single hidden lstm layer.\n",
    "    \n",
    "    Reference :\n",
    "    https://stackabuse.com/time-series-prediction-using-lstm-with-pytorch-in-python\n",
    "    https://colab.research.google.com/github/dlmacedo/starter-academic/blob/master/content/courses/deeplearning/notebooks/pytorch/Time_Series_Prediction_with_LSTM_Using_PyTorch.ipynb\n",
    "    https://curiousily.com/posts/time-series-forecasting-with-lstm-for-daily-coronavirus-cases/\n",
    "    https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/recurrent_neural_network/main.py#L39-L58\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, num_layers=1, batch_size=1):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Instantiate model layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            the number of features in the input layer\n",
    "        hidden_size : int\n",
    "            the number of units (neurons) in each hidden layer\n",
    "            (single hidden lstm layer for now)\n",
    "        output_size : int, Default: 1\n",
    "            the number of dimension for the output\n",
    "        \n",
    "        <value changes not recomemnded>\n",
    "        num_layers : int, Default: 1\n",
    "            Number of recurrent layers\n",
    "            (single lstm layer for now)\n",
    "        batch_size : int, Default: 1\n",
    "            size of batches\n",
    "            (No batches for now; batch_size=1)\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # the layers\n",
    "        # first layer (hidden lstm layer)\n",
    "        # https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        self.hidden_lstm = torch.nn.LSTM(input_size=input_dim, \n",
    "                                         hidden_size=hidden_dim, \n",
    "                                         # num_layers=num_layers, \n",
    "                                         bias=True,\n",
    "                                         batch_first=True)\n",
    "        # second layer (linear output layer)\n",
    "        self.output = torch.nn.Linear(hidden_dim, \n",
    "                                      output_dim, \n",
    "                                      bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Set initial hidden and cell states as zeros (stateless LSTM)\n",
    "        and forward propagate LSTM.\n",
    "\n",
    "        math::\n",
    "            \\begin{array}{ll} \\\\\n",
    "                i_t = \\sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\\\\n",
    "                f_t = \\sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\\\\n",
    "                g_t = \\tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\\\\n",
    "                o_t = \\sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\\\\n",
    "                c_t = f_t \\odot c_{t-1} + i_t \\odot g_t \\\\\n",
    "                h_t = o_t \\odot \\tanh(c_t) \\\\\n",
    "            \\end{array}\n",
    "        \n",
    "        :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell state at time `t`.\n",
    "        \n",
    "        (input, (h_0, c_0)) are the inputs and\n",
    "        (output, (h_n, c_n)) are the returns of lstm layer for t = n lstm cell.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x: inputs; shape (seq_len, batch_size=1, input_dim)\n",
    "            tensor containing the features of the input sequence. \n",
    "            must include batch_size(=1) dimension as dim=1\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        h: \n",
    "        \"\"\"\n",
    "        # reset_hidden_state (stateless LSTM)\n",
    "        # hidden state (lstm層のアクティベーション) of shape (batch, num_layers, hidden_size): batch_first=True\n",
    "        h_0 = torch.zeros(self.batch_size, 1 * self.num_layers, self.hidden_dim)\n",
    "        # cell state (メモリセル) of shape (batch, num_layers, hidden_size): batch_first=True\n",
    "        c_0 = torch.zeros(self.batch_size, 1 * self.num_layers, self.hidden_dim)\n",
    "        \n",
    "        # Forward pass\n",
    "        # input to hidden LSTM layer\n",
    "        # out: tensor of shape (batch_size, seq_length, hidden_size): batch_first=True\n",
    "        h_all_time, (h_latest, c_latest) = self.hidden_lstm(x, (h_0, c_0))\n",
    "        \n",
    "        # change shape of h_all_time from (batch, num_layers, hidden_size) to (batch*num_layers, hidden_size)\n",
    "        h = h_latest.view(-1, self.hidden_dim)\n",
    "        \n",
    "        # hidden LSTM layer to output layer\n",
    "        out = self.output(h)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# import internal modules\n",
    "sys.path.insert(1, '../')\n",
    "from models.nn import MLP\n",
    "from utils.data_editor import lag, train_test_split\n",
    "\n",
    "# set seeds for reproductibility\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Prepare Data --> 関数\n",
    "\n",
    "# read processed data\n",
    "df = pd.read_csv(\"../../dataset/processed/dataset.csv\")\n",
    "\n",
    "# save column names\n",
    "earning_v = df.columns[4: 10].values\n",
    "account_v_bs = df.columns[11:].values\n",
    "account_v_pl = df.columns[10:11].values\n",
    "\n",
    "# y: \"１株当たり利益［３ヵ月］\"\n",
    "y = df[earning_v[-1]]\n",
    "\n",
    "# x: ['棚卸資産', '資本的支出', '期末従業員数', '受取手形・売掛金／売掛金及びその他の短期債権', \n",
    "#     '販売費及び一般管理費']\n",
    "x = df[np.append(account_v_bs, account_v_pl)]\n",
    "\n",
    "# Unlike MLP, LSTM needs to prepare lagged inputs with seq_len matrix.\n",
    "# feature must be lag1 (y||x)\n",
    "num_lag = 1\n",
    "y_lag = lag(y, num_lag, drop_nan=False, reset_index=False)\n",
    "x_lag = lag(x, num_lag, drop_nan=False, reset_index=False)\n",
    "\n",
    "# Redefine data name as target (y) and feature (y_lag and x_lag)\n",
    "target = y\n",
    "feature = pd.concat([y_lag, x_lag], axis=1)\n",
    "\n",
    "# time series train test split (4/5) : (1/5), yearly bases\n",
    "# DataLoader使うからTrainとtestぶつ切りにしたらtest時サンプル減るんじゃね?\n",
    "target_train, target_test = train_test_split(target, ratio=(4,1))\n",
    "feature_train, feature_test = train_test_split(feature, ratio=(4,1))\n",
    "\n",
    "train_date = df[\"決算期\"][target_train.index] # for plotting !!!! <-- 改善の余地あり, targetはtensorになってindexがなくなるから\n",
    "test_date = df[\"決算期\"][target_test.index] # for y_hat index !!!! <-- 改善の余地あり, targetはtensorになってindexがなくなるから"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 6) (56,)\n",
      "(16, 6) (16,)\n"
     ]
    }
   ],
   "source": [
    "print(feature_train.shape, target_train.shape)\n",
    "print(feature_test.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add to feature_test\n",
      "    １株当たり利益_lag1  棚卸資産_lag1  資本的支出_lag1  期末従業員数_lag1  \\\n",
      "53        205.41  2201802.0   1594721.0     349131.0   \n",
      "54        193.98  2195186.0   3079472.0     349508.0   \n",
      "55        202.05  2104725.0   3773177.0     349766.0   \n",
      "\n",
      "    受取手形・売掛金／売掛金及びその他の短期債権_lag1  販売費及び一般管理費_lag1  \n",
      "53                    1922211.0         674482.0  \n",
      "54                    1988350.0         726063.0  \n",
      "55                    1915883.0         730674.0  \n",
      "add to target_test\n",
      "53    193.98\n",
      "54    202.05\n",
      "55    139.92\n",
      "Name: １株当たり利益, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# add time length of \"seq_len - 1\" to test (for seq_len DataLoader)\n",
    "seq_len = 4 # training_window for one step prediction ## HYPARAM\n",
    "\n",
    "print(\"add to feature_test\")\n",
    "print(feature_train[-(seq_len-1) :])\n",
    "print(\"add to target_test\")\n",
    "print(target_train[-(seq_len-1) :])\n",
    "\n",
    "feature_test = pd.concat([feature_train[-(seq_len-1) :], feature_test], axis=0)\n",
    "target_test = pd.concat([target_train[-(seq_len-1) :], target_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(56, 6) (56,)\n",
      "(19, 6) (19,)\n"
     ]
    }
   ],
   "source": [
    "print(feature_train.shape, target_train.shape)\n",
    "print(feature_test.shape, target_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop nan in train data head caused by lag()\n",
    "feature_train = feature_train.dropna(axis=0)\n",
    "target_train = target_train[feature_train.index]\n",
    "\n",
    "# setting torch\n",
    "dtype = torch.float # double float problem in layer \n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Make data to torch.tensor\n",
    "target_train = torch.tensor(target_train.values, dtype=dtype)\n",
    "feature_train = torch.tensor(feature_train.values, dtype=dtype)\n",
    "target_test = torch.tensor(target_test.values, dtype=dtype)\n",
    "feature_test = torch.tensor(feature_test.values, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([55, 6]) torch.Size([55])\n",
      "torch.Size([19, 6]) torch.Size([19])\n"
     ]
    }
   ],
   "source": [
    "print(feature_train.size(), target_train.size())\n",
    "print(feature_test.size(), target_test.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unlike MLP, LSTM needs to prepare lagged inputs with seq_len matrix.\n",
    "# inherit from the torch.utils.data.Dataset class\n",
    "class TimeseriesDataset(torch.utils.data.Dataset):   \n",
    "    \"\"\"\n",
    "    Torch based time-series dataset object class.\n",
    "    This object could be the input for torch.utils.data.DataLoader()\n",
    "    \n",
    "    https://stackoverflow.com/questions/57893415/pytorch-dataloader-for-time-series-task\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_lag1, target, seq_len=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        feature_lag1 : torch.tensor\n",
    "            explanatory variables matrix with only 1 lag.\n",
    "        feature_lag1 : torch.tensor\n",
    "            explained variable vector with no lag.\n",
    "        seq_len : int\n",
    "            There're so many names for this.\n",
    "                * rolling window size\n",
    "                * sliding window size\n",
    "                * training window size\n",
    "                * sequence length\n",
    "            Overall, this is the number of window lags of inputs for a single forward prediction.\n",
    "        \"\"\"\n",
    "        self.feature_lag1 = feature_lag1\n",
    "        self.target = target\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.feature_lag1.__len__() - (self.seq_len-1)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.feature_lag1[index:index+self.seq_len], self.target[index+self.seq_len-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "\n",
    "train_dataset = TimeseriesDataset(feature_train, target_train, seq_len=seq_len)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = 1, shuffle = False)\n",
    "# for t, (feature_t, target_t) in enumerate(train_loader):\n",
    "#     print(t, feature_t.size(), target_t.size())\n",
    "#     print(t, feature_t, target_t)\n",
    "# print(\"######## End Train ########\")\n",
    "    \n",
    "test_dataset = TimeseriesDataset(feature_test, target_test, seq_len=seq_len)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = 1, shuffle = False)\n",
    "# for t, (feature_t, target_t) in enumerate(test_loader):\n",
    "#     print(t, feature_t.size(), target_t.size())\n",
    "#     print(t, feature_t, target_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルのinstanciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (hidden_lstm): LSTM(6, 100, batch_first=True)\n",
       "  (output): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = feature_train.size()[1] # 特徴量数\n",
    "hidden_dim = 100 # hidden unit数\n",
    "num_layers = 1 # rnn hidden layer数\n",
    "batch_size = 1 # ミニバッチしたくないからバッチサイズ=1でいいってことだよね?(塊でやらない)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "lstm = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=1)\n",
    "lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの学習・訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 962.9001\n",
      "Epoch [200/1000], Loss: 261.3388\n",
      "Epoch [300/1000], Loss: 83.6308\n",
      "Epoch [400/1000], Loss: 18.2971\n",
      "Epoch [500/1000], Loss: 0.7993\n",
      "Epoch [600/1000], Loss: 0.0015\n",
      "Epoch [700/1000], Loss: 0.0001\n",
      "Epoch [800/1000], Loss: 0.0013\n",
      "Epoch [900/1000], Loss: 0.0014\n",
      "Epoch [1000/1000], Loss: 0.0014\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.005\n",
    "num_epochs = 1000\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "total_step = len(feature_train) # バッチなしだと時系列の長さseq_lenと同じ?\n",
    "for epoch in range(num_epochs):\n",
    "    # LSTMはtを1つずつ進めて学習?\n",
    "    for t, (feature_t, target_t) in enumerate(train_loader):\n",
    "        # feature_t; x_t (seq_len x D_x)\n",
    "        # torch.nn.LSTM() に渡すために変形 input: (seq_len, batch, input_size)\n",
    "#         feature_t = feature_t.view(1, seq_len, input_dim) # (1, 1, 24)ではなく(batch, seq_len, input_size) = (1, 4, 6): batch_size=True\n",
    "        \n",
    "        # Forward pass\n",
    "        target_t_pred = lstm(feature_t)\n",
    "        loss = criterion(target_t_pred.view(1), target_t) # size()をtorch.Size([1])にそろえる\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#         print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "#                .format(epoch+1, num_epochs, t+1, total_step, loss.item()))\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print ('Epoch [{}/{}], Loss: {:.4f}' .format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(lstm.state_dict(), '../../assets/trained_models/_lstm_prototype.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## モデルの評価(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = LSTM(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=1)\n",
    "model.load_state_dict(torch.load(\"../../assets/trained_models/_lstm_prototype.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (hidden_lstm): LSTM(6, 100, batch_first=True)\n",
       "  (output): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_hat_lstm = []\n",
    "with torch.no_grad():\n",
    "    for feature_t, target_t in test_loader:\n",
    "        y_hat = model(feature_t)\n",
    "        y_hat_lstm.append(y_hat.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "決算期\n",
       "2016-06-01    118.892754\n",
       "2016-09-01     29.729053\n",
       "2016-12-01     -9.026224\n",
       "2017-03-01     70.719826\n",
       "2017-06-01    118.939949\n",
       "2017-09-01     57.563999\n",
       "2017-12-01    100.504547\n",
       "2018-03-01    104.691216\n",
       "2018-06-01    120.140800\n",
       "2018-09-01     57.563999\n",
       "2018-12-01     52.551453\n",
       "2019-03-01     96.856369\n",
       "2019-06-01    120.475082\n",
       "2019-09-01     57.791954\n",
       "2019-12-01    118.219749\n",
       "2020-03-01    109.860718\n",
       "Name: y_hat_lstm_prptotype, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to DataFrame and save as csv\n",
    "y_hat_lstm = pd.Series(y_hat_lstm)\n",
    "\n",
    "model_name = \"lstm_prptotype\"\n",
    "y_hat_lstm.name = 'y_hat_' + model_name\n",
    "y_hat_lstm.index = test_date\n",
    "\n",
    "y_hat_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat_lstm.to_csv('../../assets/y_hats/y_hat_' + model_name + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
