{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external libraries\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "\n",
    "# import internal modules\n",
    "sys.path.insert(1, '../src/')\n",
    "from models.nn import MLP, TimeSeriesDataset\n",
    "from utils.data_editor import lag, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あらた\n",
      "test window:  12\n",
      "train window:  36\n",
      "len of train dataset:  12\n",
      "rolling window:  0\n",
      "step 0: loss 2217.675537109375\n",
      "step 1: loss 2070.275634765625\n",
      "step 100: loss 69.08267211914062\n",
      "step 499: loss 0.8574444651603699\n",
      "rolling window:  1\n",
      "step 0: loss 5.491274833679199\n",
      "step 1: loss 23.569185256958008\n",
      "step 100: loss 0.5542507767677307\n",
      "step 499: loss 0.14426058530807495\n",
      "rolling window:  2\n",
      "step 0: loss 1.4999802112579346\n",
      "step 1: loss 0.5064970850944519\n",
      "step 100: loss 0.11254584789276123\n",
      "step 499: loss 0.030913986265659332\n",
      "rolling window:  3\n",
      "step 0: loss 0.14402541518211365\n",
      "step 1: loss 0.22435027360916138\n",
      "step 100: loss 0.029584594070911407\n",
      "step 499: loss 0.004174630623310804\n",
      "rolling window:  4\n",
      "step 0: loss 12.0000638961792\n",
      "step 1: loss 12.407329559326172\n",
      "step 100: loss 0.0552961491048336\n",
      "step 499: loss 8.779268682701513e-06\n",
      "rolling window:  5\n",
      "step 0: loss 0.16224625706672668\n",
      "step 1: loss 0.08104564249515533\n",
      "step 100: loss 1.3870835573470686e-05\n",
      "step 499: loss 4.903565054004133e-11\n",
      "rolling window:  6\n",
      "step 0: loss 2.066044330596924\n",
      "step 1: loss 0.9792414903640747\n",
      "step 100: loss 0.003256978001445532\n",
      "step 499: loss 2.830305234624575e-08\n",
      "rolling window:  7\n",
      "step 0: loss 0.4434582591056824\n",
      "step 1: loss 0.3619387745857239\n",
      "step 100: loss 0.006821556482464075\n",
      "step 499: loss 7.290111569968616e-11\n",
      "rolling window:  8\n",
      "step 0: loss 16.298601150512695\n",
      "step 1: loss 12.707640647888184\n",
      "step 100: loss 5.1336541175842285\n",
      "step 499: loss 0.13155505061149597\n",
      "rolling window:  9\n",
      "step 0: loss 36.19691848754883\n",
      "step 1: loss 32.258182525634766\n",
      "step 100: loss 7.233221054077148\n",
      "step 499: loss 0.2829189896583557\n",
      "rolling window:  10\n",
      "step 0: loss 25.916532516479492\n",
      "step 1: loss 19.914785385131836\n",
      "step 100: loss 8.53415298461914\n",
      "step 499: loss 0.2617267966270447\n",
      "rolling window:  11\n",
      "step 0: loss 37.57020568847656\n",
      "step 1: loss 36.169254302978516\n",
      "step 100: loss 1.5772100687026978\n",
      "step 499: loss 0.0065822843462228775\n"
     ]
    }
   ],
   "source": [
    "# read processed data\n",
    "df = pd.read_csv(\"../data/processed/tidy_df.csv\", index_col=[0, 1, 2])\n",
    "\n",
    "# empty list for dataframes\n",
    "y_test_list = []\n",
    "y_hat_umlp = []\n",
    "\n",
    "i = df.index.get_level_values(0).unique()[0]\n",
    "print(i)\n",
    "\n",
    "# y : \"EPS\"\n",
    "y = df.loc[pd.IndexSlice[i, :, :], \"EPS\"]\n",
    "\n",
    "# x, exogenous regressors : 'INV', 'AR', 'CAPX', 'GM', 'SA', 'ETR', 'LF'\n",
    "#     x = df.loc[pd.IndexSlice[i, :, :], ['INV', 'AR', 'CAPX', 'GM', 'SA', 'ETR', 'LF']]\n",
    "\n",
    "# Unlike statsmodel SARIMA package, NN needs to prepare lagged inputs manually if needed.\n",
    "# y_lag and x_lag (lag 4 for now)\n",
    "num_lag = 4\n",
    "y_lag = lag(y, num_lag, drop_nan=False, reset_index=False)\n",
    "#     x_lag = lag(x, num_lag, drop_nan=False, reset_index=False)\n",
    "\n",
    "# Redefine data name as target (y) and feature (y_lag) (explanatory variable, predictor)\n",
    "target = y\n",
    "feature = y_lag\n",
    "\n",
    "# save simple test data series\n",
    "_, target_test_dataset = train_test_split(target, ratio=(4,1))\n",
    "_, feature_test_dataset = train_test_split(feature, ratio=(4,1))\n",
    "\n",
    "# drop nan caused by lag()\n",
    "feature = feature.dropna(axis=0)\n",
    "target = target[feature.index]\n",
    "\n",
    "# setting torch\n",
    "dtype = torch.float # double float problem in layer \n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Make data to torch.tensor\n",
    "target = torch.tensor(target.values, dtype=dtype)\n",
    "feature = torch.tensor(feature.values, dtype=dtype)\n",
    "target_test_dataset = torch.tensor(target_test_dataset.values, dtype=dtype)\n",
    "feature_test_dataset = torch.tensor(feature_test_dataset.values, dtype=dtype)\n",
    "\n",
    "# rolling window data preparation\n",
    "\n",
    "### ! Hyper-Parameter ! ##########################################################\n",
    "# all period: 48, train 36, test 12\n",
    "test_window = len(target_test_dataset)\n",
    "print(\"test window: \", test_window)\n",
    "\n",
    "train_window = len(target) - test_window\n",
    "print(\"train window: \", train_window)\n",
    "##################################################################################\n",
    "\n",
    "train_dataset = TimeSeriesDataset(feature, target, train_window)\n",
    "print(\"len of train dataset: \", len(train_dataset))\n",
    "#     len(train_dataset) == len(target) - train_window = 48 - 36 = 12 == test_window\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### ! Hyper-Parameter ! ##########################################################\n",
    "num_epochs = 500\n",
    "learning_rate = 1e-2\n",
    "input_units = 1\n",
    "hidden_units = 512\n",
    "# num_layers = 1\n",
    "output_units = 1\n",
    "# Optimizer\n",
    "\n",
    "model_name = 'umlp' + '_hid' + str(hidden_units) + '_lr' + str(learning_rate) + '_epo' + str(num_epochs)\n",
    "##################################################################################\n",
    "\n",
    "# load rolling window data flow\n",
    "for num_window, (feature_train, target_train) in enumerate(train_loader):\n",
    "    print(\"rolling window: \", num_window)\n",
    "    feature_train = feature_train[0] # extract single batch\n",
    "    target_train = target_train[0] # extract single batch\n",
    "\n",
    "#     #####Init the Model #######################\n",
    "#     mlp = MLP(input_features=feature_train.size()[1], hidden_units=hidden_units, output_units=output_units)\n",
    "#     ##### Set Criterion Optimzer and scheduler ####################\n",
    "#     criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "#     optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate) # link to mlp parameters (lr should be 1e-2)    \n",
    "    \n",
    "    #(only first window)\n",
    "    if num_window == 0:\n",
    "        #####Init the Model #######################\n",
    "        mlp = MLP(input_features=feature_train.size()[1], hidden_units=hidden_units, output_units=output_units)\n",
    "        ##### Set Criterion Optimzer and scheduler ####################\n",
    "        criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate) # link to mlp parameters (lr should be 1e-2)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Train the model: Learning iteration\n",
    "    # use pre window trained model's weight as initial weight (continue training)\n",
    "    #             print(\"initial weight: \", mlp.state_dict()['hidden.weight'][0])\n",
    "\n",
    "    for step in range(num_epochs):\n",
    "        # Forward pass\n",
    "        target_pred = mlp(feature_train)\n",
    "        # let y_pred be the same size as y\n",
    "        target_pred = target_pred.squeeze(1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(target_pred, target_train) # link to mlp output\n",
    "        if (step == 0) | (step == 1) | (step == 100) | (step == 499):\n",
    "            print(f\"step {step}: loss {loss.item()}\")\n",
    "\n",
    "        # Zero gradients, perform backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save the trained model\n",
    "#     PATH = '../../assets/trained_models/univariate/mlp/' + model_name + \"_\" + i + \"_\" + \"win\" + str(num_window) + '.pth'\n",
    "#     torch.save(mlp.state_dict(), PATH)\n",
    "    # use the existing trained model and continute training next window.\n",
    "    #             print(\"inherit weight: \", mlp.state_dict()['hidden.weight'][0])\n",
    "\n",
    "    # predict y_hat (target_hat) <- 良くないかも、with torch_nograd() と model.eval()\n",
    "    with torch.no_grad():\n",
    "        target_test = target_test_dataset[num_window]\n",
    "        feature_test = feature_test_dataset[num_window]\n",
    "        y_hat_umlp.append(mlp(feature_test).squeeze().detach().numpy())\n",
    "    #                 print(feature_test)\n",
    "    #                 print(target_test)\n",
    "    #                 print(y_hat_umlp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.922597908539892"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(y.iloc[-12:].values - np.array(y_hat_umlp)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あらた\n",
      "test window:  12\n",
      "train window:  36\n",
      "len of train dataset:  12\n",
      "WINDOW:  0\n",
      "epoch 0: loss 6991.52685546875\n",
      "epoch 1: loss 5212.80419921875\n",
      "epoch 100: loss 63.719154357910156\n",
      "epoch 499: loss 20.02129554748535\n",
      "WINDOW:  1\n",
      "epoch 0: loss 49.18145751953125\n",
      "epoch 1: loss 23.821441650390625\n",
      "epoch 100: loss 110.38540649414062\n",
      "epoch 499: loss 19.06254768371582\n",
      "WINDOW:  2\n",
      "epoch 0: loss 136.7188262939453\n",
      "epoch 1: loss 63.97145080566406\n",
      "epoch 100: loss 49.73845291137695\n",
      "epoch 499: loss 6.5564351081848145\n",
      "WINDOW:  3\n",
      "epoch 0: loss 61.90725326538086\n",
      "epoch 1: loss 46.753807067871094\n",
      "epoch 100: loss 13.831600189208984\n",
      "epoch 499: loss 31.53185272216797\n",
      "WINDOW:  4\n",
      "epoch 0: loss 328.1629638671875\n",
      "epoch 1: loss 142.90606689453125\n",
      "epoch 100: loss 89.15724182128906\n",
      "epoch 499: loss 19.888721466064453\n",
      "WINDOW:  5\n",
      "epoch 0: loss 28.871501922607422\n",
      "epoch 1: loss 33.40175247192383\n",
      "epoch 100: loss 8.478342056274414\n",
      "epoch 499: loss 79.77935791015625\n",
      "WINDOW:  6\n",
      "epoch 0: loss 137.2894287109375\n",
      "epoch 1: loss 126.72782897949219\n",
      "epoch 100: loss 17.107370376586914\n",
      "epoch 499: loss 51.66316604614258\n",
      "WINDOW:  7\n",
      "epoch 0: loss 59.790279388427734\n",
      "epoch 1: loss 52.238563537597656\n",
      "epoch 100: loss 54.3714714050293\n",
      "epoch 499: loss 110.40282440185547\n",
      "WINDOW:  8\n",
      "epoch 0: loss 49.97850036621094\n",
      "epoch 1: loss 43.111385345458984\n",
      "epoch 100: loss 87.58363342285156\n",
      "epoch 499: loss 42.50239181518555\n",
      "WINDOW:  9\n",
      "epoch 0: loss 975.020751953125\n",
      "epoch 1: loss 574.280029296875\n",
      "epoch 100: loss 436.701904296875\n",
      "epoch 499: loss 315.66204833984375\n",
      "WINDOW:  10\n",
      "epoch 0: loss 750.8316650390625\n",
      "epoch 1: loss 733.6951293945312\n",
      "epoch 100: loss 499.30517578125\n",
      "epoch 499: loss 902.3160400390625\n",
      "WINDOW:  11\n",
      "epoch 0: loss 1738.681640625\n",
      "epoch 1: loss 1512.9453125\n",
      "epoch 100: loss 841.059326171875\n",
      "epoch 499: loss 239.36538696289062\n"
     ]
    }
   ],
   "source": [
    "# read processed data\n",
    "df = pd.read_csv(\"../data/processed/tidy_df.csv\", index_col=[0, 1, 2])\n",
    "\n",
    "# empty list for dataframes\n",
    "y_test_list = []\n",
    "y_hat_umlp = []\n",
    "\n",
    "i = df.index.get_level_values(0).unique()[0]\n",
    "print(i)\n",
    "\n",
    "# y : \"EPS\"\n",
    "y = df.loc[pd.IndexSlice[i, :, :], \"EPS\"]\n",
    "\n",
    "# x, exogenous regressors : 'INV', 'AR', 'CAPX', 'GM', 'SA', 'ETR', 'LF'\n",
    "#     x = df.loc[pd.IndexSlice[i, :, :], ['INV', 'AR', 'CAPX', 'GM', 'SA', 'ETR', 'LF']]\n",
    "\n",
    "# Unlike statsmodel SARIMA package, NN needs to prepare lagged inputs manually if needed.\n",
    "# y_lag and x_lag (lag 4 for now)\n",
    "num_lag = 4\n",
    "y_lag = lag(y, num_lag, drop_nan=False, reset_index=False)\n",
    "#     x_lag = lag(x, num_lag, drop_nan=False, reset_index=False)\n",
    "\n",
    "# Redefine data name as target (y) and feature (y_lag) (explanatory variable, predictor)\n",
    "target = y\n",
    "feature = y_lag\n",
    "\n",
    "# save simple test data series\n",
    "_, target_test_dataset = train_test_split(target, ratio=(4,1))\n",
    "_, feature_test_dataset = train_test_split(feature, ratio=(4,1))\n",
    "\n",
    "# drop nan caused by lag()\n",
    "feature = feature.dropna(axis=0)\n",
    "target = target[feature.index]\n",
    "\n",
    "# setting torch\n",
    "dtype = torch.float # double float problem in layer \n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Make data to torch.tensor\n",
    "target = torch.tensor(target.values, dtype=dtype)\n",
    "feature = torch.tensor(feature.values, dtype=dtype)\n",
    "target_test_dataset = torch.tensor(target_test_dataset.values, dtype=dtype)\n",
    "feature_test_dataset = torch.tensor(feature_test_dataset.values, dtype=dtype)\n",
    "\n",
    "# rolling window data preparation\n",
    "\n",
    "### ! Hyper-Parameter ! ##########################################################\n",
    "# all period: 48, train 36, test 12\n",
    "test_window = len(target_test_dataset)\n",
    "print(\"test window: \", test_window)\n",
    "\n",
    "train_window = len(target) - test_window\n",
    "print(\"train window: \", train_window)\n",
    "##################################################################################\n",
    "\n",
    "train_dataset = TimeSeriesDataset(feature, target, train_window)\n",
    "print(\"len of train dataset: \", len(train_dataset))\n",
    "#     len(train_dataset) == len(target) - train_window = 48 - 36 = 12 == test_window\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### ! Hyper-Parameter ! ##########################################################\n",
    "num_epochs = 500\n",
    "learning_rate = 1e-2\n",
    "input_units = 1\n",
    "hidden_units = 512\n",
    "# num_layers = 1\n",
    "output_units = 1\n",
    "# Optimizer\n",
    "batch_size = 2\n",
    "\n",
    "model_name = 'umlp' + '_hid' + str(hidden_units) + '_lr' + str(learning_rate) + '_epo' + str(num_epochs)\n",
    "##################################################################################\n",
    "\n",
    "# load rolling window data flow\n",
    "for num_window, (feature_train_all_batch, target_train_all_batch) in enumerate(train_loader):\n",
    "    print(\"WINDOW: \", num_window)\n",
    "#     print(feature_train_all_batch.size(), target_train_all_batch.size())\n",
    "    feature_train_all_batch = feature_train_all_batch[0] # extract single batch\n",
    "    target_train_all_batch = target_train_all_batch[0] # extract single batch\n",
    "\n",
    "#     #####Init the Model #######################\n",
    "#     mlp = MLP(input_features=feature_train.size()[1], hidden_units=hidden_units, output_units=output_units)\n",
    "#     ##### Set Criterion Optimzer and scheduler ####################\n",
    "#     criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "#     optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate) # link to mlp parameters (lr should be 1e-2)\n",
    "    \n",
    "    #(only first window)\n",
    "    if num_window == 0:\n",
    "        #####Init the Model #######################\n",
    "        mlp = MLP(input_features=feature_train.size()[1], hidden_units=hidden_units, output_units=output_units)\n",
    "        ##### Set Criterion Optimzer and scheduler ####################\n",
    "        criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate) # link to mlp parameters (lr should be 1e-2)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # mini-batch loader\n",
    "    batch_dataset = TimeSeriesDataset(feature_train_all_batch, target_train_all_batch, train_window=None)\n",
    "    minibatch_loader = torch.utils.data.DataLoader(batch_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Train the model: Learning iteration\n",
    "    # use pre window trained model's weight as initial weight (continue training)\n",
    "    #             print(\"initial weight: \", mlp.state_dict()['hidden.weight'][0])\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for num_mini_batch, (feature_train_mini_batch, target_train_mini_batch) in enumerate(minibatch_loader):    \n",
    "            # Forward pass\n",
    "            target_pred = mlp(feature_train_mini_batch)\n",
    "            # let y_pred be the same size as y\n",
    "            target_pred = target_pred.squeeze(1)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(target_pred, target_train_mini_batch) # link to mlp output\n",
    "\n",
    "            # Zero gradients, perform backward pass, and update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch == 0) | (epoch == 1) | (epoch == 100) | (epoch == 499):\n",
    "            print(f\"epoch {epoch}: loss {loss.item()}\")\n",
    "\n",
    "    # Save the trained model\n",
    "#     PATH = '../../assets/trained_models/univariate/mlp/' + model_name + \"_\" + i + \"_\" + \"win\" + str(num_window) + '.pth'\n",
    "#     torch.save(mlp.state_dict(), PATH)\n",
    "    # use the existing trained model and continute training next window.\n",
    "    #             print(\"inherit weight: \", mlp.state_dict()['hidden.weight'][0])\n",
    "\n",
    "    # predict y_hat (target_hat) <- 良くないかも、with torch_nograd() と model.eval()\n",
    "    with torch.no_grad():\n",
    "        target_test = target_test_dataset[num_window]\n",
    "        feature_test = feature_test_dataset[num_window]\n",
    "        y_hat_umlp.append(mlp(feature_test).squeeze().detach().numpy())\n",
    "    #                 print(feature_test)\n",
    "    #                 print(target_test)\n",
    "    #                 print(y_hat_umlp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19.519032412730972"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(y.iloc[-12:].values - np.array(y_hat_umlp)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
