{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import external libraries\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import math\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# import internal modules\n",
    "sys.path.insert(1, '../src/')\n",
    "from models.nn import MLP, TimeSeriesDataset\n",
    "from utils.data_editor import lag, train_test_split\n",
    "from utils.accuracy import MAE, MAPE, MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ＷＤＢホールディングス\n",
      "test window:  12\n",
      "train window:  36\n",
      "len of train dataset:  12\n",
      "rolling window:  0\n",
      "step 0: loss 9314380.0\n",
      "step 1: loss 9312439.0\n",
      "step 100: loss 9119387.0\n",
      "step 499: loss 8409325.0\n",
      "step 5000: loss 2917395.75\n",
      "step 9999: loss 1732541.375\n",
      "rolling window:  1\n",
      "step 0: loss 1683435.375\n",
      "step 1: loss 1684772.75\n",
      "step 100: loss 1676189.5\n",
      "step 499: loss 1625735.5\n",
      "step 5000: loss 1359126.875\n",
      "step 9999: loss 1214809.625\n",
      "rolling window:  2\n",
      "step 0: loss 1215475.875\n",
      "step 1: loss 1222420.375\n",
      "step 100: loss 1495302.75\n",
      "step 499: loss 1416631.125\n",
      "step 5000: loss 1168318.375\n",
      "step 9999: loss 1060579.375\n",
      "rolling window:  3\n",
      "step 0: loss 1060599.625\n",
      "step 1: loss 1060565.5\n",
      "step 100: loss 1111067.25\n",
      "step 499: loss 1091223.75\n",
      "step 5000: loss 1114240.75\n",
      "step 9999: loss 1025016.4375\n",
      "rolling window:  4\n",
      "step 0: loss 1025472.5625\n",
      "step 1: loss 1025207.4375\n",
      "step 100: loss 1103348.375\n",
      "step 499: loss 1044879.4375\n",
      "step 5000: loss 964406.4375\n",
      "step 9999: loss 996024.3125\n",
      "rolling window:  5\n",
      "step 0: loss 995641.125\n",
      "step 1: loss 995602.125\n",
      "step 100: loss 994190.75\n",
      "step 499: loss 983361.3125\n",
      "step 5000: loss 935751.3125\n",
      "step 9999: loss 950189.6875\n",
      "rolling window:  6\n",
      "step 0: loss 944392.125\n",
      "step 1: loss 944372.125\n",
      "step 100: loss 959933.875\n",
      "step 499: loss 971367.5625\n",
      "step 5000: loss 899520.0625\n",
      "step 9999: loss 896898.25\n",
      "rolling window:  7\n",
      "step 0: loss 871335.3125\n",
      "step 1: loss 887898.25\n",
      "step 100: loss 877593.25\n",
      "step 499: loss 840266.75\n",
      "step 5000: loss 859056.6875\n",
      "step 9999: loss 799411.125\n",
      "rolling window:  8\n",
      "step 0: loss 759198.625\n",
      "step 1: loss 768749.625\n",
      "step 100: loss 791635.0\n",
      "step 499: loss 802610.75\n",
      "step 5000: loss 745110.75\n",
      "step 9999: loss 721759.5625\n",
      "rolling window:  9\n",
      "step 0: loss 695414.0\n",
      "step 1: loss 695393.5625\n",
      "step 100: loss 691829.3125\n",
      "step 499: loss 683743.3125\n",
      "step 5000: loss 618735.125\n",
      "step 9999: loss 581472.9375\n",
      "rolling window:  10\n",
      "step 0: loss 544626.25\n",
      "step 1: loss 544507.5625\n",
      "step 100: loss 588179.75\n",
      "step 499: loss 579972.875\n",
      "step 5000: loss 511083.34375\n",
      "step 9999: loss 467395.15625\n",
      "rolling window:  11\n",
      "step 0: loss 435830.28125\n",
      "step 1: loss 435961.84375\n",
      "step 100: loss 569896.3125\n",
      "step 499: loss 545079.8125\n",
      "step 5000: loss 472540.875\n",
      "step 9999: loss 409250.90625\n"
     ]
    }
   ],
   "source": [
    "# read processed data\n",
    "df = pd.read_csv(\"../data/processed/tidy_df.csv\", index_col=[0, 1, 2])\n",
    "\n",
    "# empty list for dataframes\n",
    "y_test_list = []\n",
    "y_hat_umlp = []\n",
    "\n",
    "i = df.index.get_level_values(0).unique()[-4]\n",
    "print(i)\n",
    "\n",
    "# y : \"EPS\"\n",
    "y = df.loc[pd.IndexSlice[i, :, :], \"EPS\"]\n",
    "\n",
    "# x, exogenous regressors : 'INV', 'AR', 'CAPX', 'GM', 'SA', 'ETR', 'LF'\n",
    "#     x = df.loc[pd.IndexSlice[i, :, :], ['INV', 'AR', 'CAPX', 'GM', 'SA', 'ETR', 'LF']]\n",
    "\n",
    "# Unlike statsmodel SARIMA package, NN needs to prepare lagged inputs manually if needed.\n",
    "# y_lag and x_lag (lag 4 for now)\n",
    "num_lag = 4\n",
    "y_lag = lag(y, num_lag, drop_nan=False, reset_index=False)\n",
    "#     x_lag = lag(x, num_lag, drop_nan=False, reset_index=False)\n",
    "\n",
    "# Redefine data name as target (y) and feature (y_lag) (explanatory variable, predictor)\n",
    "target = y\n",
    "feature = y_lag\n",
    "\n",
    "# save simple test data series\n",
    "_, target_test_dataset = train_test_split(target, ratio=(4,1))\n",
    "_, feature_test_dataset = train_test_split(feature, ratio=(4,1))\n",
    "\n",
    "# drop nan caused by lag()\n",
    "feature = feature.dropna(axis=0)\n",
    "target = target[feature.index]\n",
    "\n",
    "# setting torch\n",
    "dtype = torch.float # double float problem in layer \n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Make data to torch.tensor\n",
    "target = torch.tensor(target.values, dtype=dtype)\n",
    "feature = torch.tensor(feature.values, dtype=dtype)\n",
    "target_test_dataset = torch.tensor(target_test_dataset.values, dtype=dtype)\n",
    "feature_test_dataset = torch.tensor(feature_test_dataset.values, dtype=dtype)\n",
    "\n",
    "# rolling window data preparation\n",
    "\n",
    "### ! Hyper-Parameter ! ##########################################################\n",
    "# all period: 48, train 36, test 12\n",
    "test_window = len(target_test_dataset)\n",
    "print(\"test window: \", test_window)\n",
    "\n",
    "train_window = len(target) - test_window\n",
    "print(\"train window: \", train_window)\n",
    "##################################################################################\n",
    "\n",
    "train_dataset = TimeSeriesDataset(feature, target, train_window)\n",
    "print(\"len of train dataset: \", len(train_dataset))\n",
    "#     len(train_dataset) == len(target) - train_window = 48 - 36 = 12 == test_window\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### ! Hyper-Parameter ! ##########################################################\n",
    "num_epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "input_units = 1\n",
    "hidden_units = 1000\n",
    "# num_layers = 1\n",
    "output_units = 1\n",
    "# Optimizer\n",
    "\n",
    "model_name = 'umlp' + '_hid' + str(hidden_units) + '_lr' + str(learning_rate) + '_epo' + str(num_epochs)\n",
    "##################################################################################\n",
    "\n",
    "# load rolling window data flow\n",
    "for num_window, (feature_train, target_train) in enumerate(train_loader):\n",
    "    print(\"rolling window: \", num_window)\n",
    "    feature_train = feature_train[0] # extract single batch\n",
    "    target_train = target_train[0] # extract single batch\n",
    "\n",
    "#     #####Init the Model #######################\n",
    "#     mlp = MLP(input_features=feature_train.size()[1], hidden_units=hidden_units, output_units=output_units)\n",
    "#     ##### Set Criterion Optimzer and scheduler ####################\n",
    "#     criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "#     optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate) # link to mlp parameters (lr should be 1e-2)    \n",
    "    \n",
    "    #(only first window)\n",
    "    if num_window == 0:\n",
    "        #####Init the Model #######################\n",
    "        mlp = MLP(input_features=feature_train.size()[1], hidden_units=hidden_units, output_units=output_units)\n",
    "        ##### Set Criterion Optimzer and scheduler ####################\n",
    "        criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate) # link to mlp parameters (lr should be 1e-2)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Train the model: Learning iteration\n",
    "    # use pre window trained model's weight as initial weight (continue training)\n",
    "    #             print(\"initial weight: \", mlp.state_dict()['hidden.weight'][0])\n",
    "\n",
    "    for step in range(num_epochs):\n",
    "        # Forward pass\n",
    "        target_pred = mlp(feature_train)\n",
    "        # let y_pred be the same size as y\n",
    "        target_pred = target_pred.squeeze(1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(target_pred, target_train) # link to mlp output\n",
    "        if (step == 0) | (step == 1) | (step == 100) | (step == 499) | (step == 5000) | (step == num_epochs-1):\n",
    "            print(f\"step {step}: loss {loss.item()}\")\n",
    "\n",
    "        # Zero gradients, perform backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save the trained model\n",
    "#     PATH = '../../assets/trained_models/univariate/mlp/' + model_name + \"_\" + i + \"_\" + \"win\" + str(num_window) + '.pth'\n",
    "#     torch.save(mlp.state_dict(), PATH)\n",
    "    # use the existing trained model and continute training next window.\n",
    "    #             print(\"inherit weight: \", mlp.state_dict()['hidden.weight'][0])\n",
    "\n",
    "    # predict y_hat (target_hat) <- 良くないかも、with torch_nograd() と model.eval()\n",
    "    with torch.no_grad():\n",
    "        target_test = target_test_dataset[num_window]\n",
    "        feature_test = feature_test_dataset[num_window]\n",
    "        y_hat_umlp.append(mlp(feature_test).squeeze().detach().numpy())\n",
    "    #                 print(feature_test)\n",
    "    #                 print(target_test)\n",
    "    #                 print(y_hat_umlp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.32865"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(np.array(target_test_dataset) - np.array(y_hat_umlp)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114.32865"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(np.array(target_test_dataset), np.array(y_hat_umlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1487675"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAPE(np.array(target_test_dataset), np.array(y_hat_umlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.414862"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(np.array(target_test_dataset), np.array(y_hat_umlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ＷＤＢホールディングス\n",
      "test window:  12\n",
      "train window:  36\n",
      "len of train dataset:  12\n",
      "rolling window:  0\n",
      "step 0: loss 9314944.0\n",
      "step 1: loss 9313099.0\n",
      "step 100: loss 9096360.0\n",
      "step 499: loss 7722076.5\n",
      "step 5000: loss 1940638.25\n",
      "step 9999: loss 910220.25\n",
      "rolling window:  1\n",
      "step 0: loss 910695.0\n",
      "step 1: loss 910603.625\n",
      "step 100: loss 901470.8125\n",
      "step 499: loss 868259.25\n",
      "step 5000: loss 558945.5625\n",
      "step 9999: loss 296499.34375\n",
      "rolling window:  2\n",
      "step 0: loss 307185.25\n",
      "step 1: loss 306515.46875\n",
      "step 100: loss 293931.25\n",
      "step 499: loss 280229.125\n",
      "step 5000: loss 166083.59375\n",
      "step 9999: loss 103654.828125\n",
      "rolling window:  3\n",
      "step 0: loss 104729.8515625\n",
      "step 1: loss 104571.109375\n",
      "step 100: loss 103078.3515625\n",
      "step 499: loss 99817.5625\n",
      "step 5000: loss 68881.3671875\n",
      "step 9999: loss 42385.27734375\n",
      "rolling window:  4\n",
      "step 0: loss 48942.6953125\n",
      "step 1: loss 48319.86328125\n",
      "step 100: loss 41948.87890625\n",
      "step 499: loss 40136.25390625\n",
      "step 5000: loss 22820.26171875\n",
      "step 9999: loss 8254.861328125\n",
      "rolling window:  5\n",
      "step 0: loss 27011.240234375\n",
      "step 1: loss 25782.927734375\n",
      "step 100: loss 8071.953125\n",
      "step 499: loss 7255.287109375\n",
      "step 5000: loss 1600.3990478515625\n",
      "step 9999: loss 57.99179458618164\n",
      "rolling window:  6\n",
      "step 0: loss 8505.720703125\n",
      "step 1: loss 7681.505859375\n",
      "step 100: loss 65.37030792236328\n",
      "step 499: loss 54.535770416259766\n",
      "step 5000: loss 49.4901123046875\n",
      "step 9999: loss 48.93061828613281\n",
      "rolling window:  7\n",
      "step 0: loss 15784.82421875\n",
      "step 1: loss 14566.9775390625\n",
      "step 100: loss 66.48880767822266\n",
      "step 499: loss 49.56410598754883\n",
      "step 5000: loss 49.00685501098633\n",
      "step 9999: loss 48.72011184692383\n",
      "rolling window:  8\n",
      "step 0: loss 37145.484375\n",
      "step 1: loss 35493.17578125\n",
      "step 100: loss 221.17039489746094\n",
      "step 499: loss 58.27886199951172\n",
      "step 5000: loss 52.38201904296875\n",
      "step 9999: loss 52.062400817871094\n",
      "rolling window:  9\n",
      "step 0: loss 26007.3359375\n",
      "step 1: loss 24606.9609375\n",
      "step 100: loss 157.0348358154297\n",
      "step 499: loss 56.01992416381836\n",
      "step 5000: loss 52.516944885253906\n",
      "step 9999: loss 52.255409240722656\n",
      "rolling window:  10\n",
      "step 0: loss 24742.6875\n",
      "step 1: loss 23405.9765625\n",
      "step 100: loss 203.25596618652344\n",
      "step 499: loss 58.12446975708008\n",
      "step 5000: loss 52.69646072387695\n",
      "step 9999: loss 52.39674758911133\n",
      "rolling window:  11\n",
      "step 0: loss 39641.9375\n",
      "step 1: loss 38000.84375\n",
      "step 100: loss 295.1963195800781\n",
      "step 499: loss 82.59750366210938\n",
      "step 5000: loss 55.29730987548828\n",
      "step 9999: loss 54.97576141357422\n"
     ]
    }
   ],
   "source": [
    "# read processed data\n",
    "df = pd.read_csv(\"../data/processed/tidy_df.csv\", index_col=[0, 1, 2])\n",
    "\n",
    "# empty list for dataframes\n",
    "y_test_list = []\n",
    "y_hat_umlp = []\n",
    "\n",
    "i = df.index.get_level_values(0).unique()[-4]\n",
    "print(i)\n",
    "\n",
    "# y : \"EPS\"\n",
    "y = df.loc[pd.IndexSlice[i, :, :], \"EPS\"]\n",
    "\n",
    "# x, exogenous regressors : 'INV', 'AR', 'CAPX', 'GM', 'SA', 'ETR', 'LF'\n",
    "#     x = df.loc[pd.IndexSlice[i, :, :], ['INV', 'AR', 'CAPX', 'GM', 'SA', 'ETR', 'LF']]\n",
    "\n",
    "# Unlike statsmodel SARIMA package, NN needs to prepare lagged inputs manually if needed.\n",
    "# y_lag and x_lag (lag 4 for now)\n",
    "num_lag = 4\n",
    "y_lag = lag(y, num_lag, drop_nan=False, reset_index=False)\n",
    "#     x_lag = lag(x, num_lag, drop_nan=False, reset_index=False)\n",
    "\n",
    "# Redefine data name as target (y) and feature (y_lag) (explanatory variable, predictor)\n",
    "target = y\n",
    "feature = y_lag\n",
    "\n",
    "# save simple test data series\n",
    "_, target_test_dataset = train_test_split(target, ratio=(4,1))\n",
    "_, feature_test_dataset = train_test_split(feature, ratio=(4,1))\n",
    "\n",
    "# drop nan caused by lag()\n",
    "feature = feature.dropna(axis=0)\n",
    "target = target[feature.index]\n",
    "\n",
    "# setting torch\n",
    "dtype = torch.float # double float problem in layer \n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Make data to torch.tensor\n",
    "target = torch.tensor(target.values, dtype=dtype)\n",
    "feature = torch.tensor(feature.values, dtype=dtype)\n",
    "target_test_dataset = torch.tensor(target_test_dataset.values, dtype=dtype)\n",
    "feature_test_dataset = torch.tensor(feature_test_dataset.values, dtype=dtype)\n",
    "\n",
    "# rolling window data preparation\n",
    "\n",
    "### ! Hyper-Parameter ! ##########################################################\n",
    "# all period: 48, train 36, test 12\n",
    "test_window = len(target_test_dataset)\n",
    "print(\"test window: \", test_window)\n",
    "\n",
    "train_window = len(target) - test_window\n",
    "print(\"train window: \", train_window)\n",
    "##################################################################################\n",
    "\n",
    "train_dataset = TimeSeriesDataset(feature, target, train_window)\n",
    "print(\"len of train dataset: \", len(train_dataset))\n",
    "#     len(train_dataset) == len(target) - train_window = 48 - 36 = 12 == test_window\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "### ! Hyper-Parameter ! ##########################################################\n",
    "num_epochs = 10000\n",
    "learning_rate = 1e-3\n",
    "input_units = 1\n",
    "hidden_units = 1000\n",
    "# num_layers = 1\n",
    "output_units = 1\n",
    "# Optimizer\n",
    "\n",
    "model_name = 'umlp' + '_hid' + str(hidden_units) + '_lr' + str(learning_rate) + '_epo' + str(num_epochs)\n",
    "##################################################################################\n",
    "\n",
    "# load rolling window data flow\n",
    "for num_window, (feature_train, target_train) in enumerate(train_loader):\n",
    "    print(\"rolling window: \", num_window)\n",
    "    feature_train = feature_train[0] # extract single batch\n",
    "    target_train = target_train[0] # extract single batch\n",
    "\n",
    "    target_test = target_test_dataset[num_window] # indexing test\n",
    "    feature_test = feature_test_dataset[num_window] # indexing test\n",
    "    \n",
    "    # Normalize \n",
    "#     target_train_scaler = StandardScaler(with_mean=False, with_std=False).fit(target_train.reshape(-1, 1))\n",
    "#     target_train_scaler = StandardScaler().fit(target_train.reshape(-1, 1))\n",
    "#     target_train_std = torch.tensor(target_train_scaler.transform(target_train.reshape(-1, 1)).reshape(target_train.shape), dtype=dtype)\n",
    "#     target_test_std = torch.tensor(target_train_scaler.transform(target_test.reshape(-1, 1)).reshape(target_test.shape), dtype=dtype)\n",
    "\n",
    "#     feature_train_scaler = StandardScaler(with_mean=False, with_std=False).fit(feature_train)\n",
    "    feature_train_scaler = StandardScaler().fit(feature_train)\n",
    "    feature_train_std = torch.tensor(feature_train_scaler.transform(feature_train).reshape(feature_train.shape), dtype=dtype)\n",
    "    feature_test_std = torch.tensor(feature_train_scaler.transform(feature_test.reshape(1, -1)).reshape(feature_test.shape), dtype=dtype)\n",
    "    \n",
    "#     #####Init the Model #######################\n",
    "#     mlp = MLP(input_features=feature_train.size()[1], hidden_units=hidden_units, output_units=output_units)\n",
    "#     ##### Set Criterion Optimzer and scheduler ####################\n",
    "#     criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "#     optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate) # link to mlp parameters (lr should be 1e-2)    \n",
    "    \n",
    "    #(only first window)\n",
    "    if num_window == 0:\n",
    "        #####Init the Model #######################\n",
    "        mlp = MLP(input_features=feature_train_std.size()[1], hidden_units=hidden_units, output_units=output_units)\n",
    "        ##### Set Criterion Optimzer and scheduler ####################\n",
    "        criterion = torch.nn.MSELoss(reduction=\"mean\")\n",
    "        optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate) # link to mlp parameters (lr should be 1e-2)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Train the model: Learning iteration\n",
    "    # use pre window trained model's weight as initial weight (continue training)\n",
    "    #             print(\"initial weight: \", mlp.state_dict()['hidden.weight'][0])\n",
    "    mlp.train()\n",
    "    for step in range(num_epochs):\n",
    "        # Forward pass\n",
    "        target_pred = mlp(feature_train_std)\n",
    "        # let y_pred be the same size as y\n",
    "        target_pred = target_pred.squeeze(1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(target_pred, target_train) # link to mlp output\n",
    "        if (step == 0) | (step == 1) | (step == 100) | (step == 499) | (step == 5000) | (step == num_epochs-1):\n",
    "            print(f\"step {step}: loss {loss.item()}\")\n",
    "\n",
    "        # Zero gradients, perform backward pass, and update weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save the trained model\n",
    "#     PATH = '../../assets/trained_models/univariate/mlp/' + model_name + \"_\" + i + \"_\" + \"win\" + str(num_window) + '.pth'\n",
    "#     torch.save(mlp.state_dict(), PATH)\n",
    "    # use the existing trained model and continute training next window.\n",
    "    #             print(\"inherit weight: \", mlp.state_dict()['hidden.weight'][0])\n",
    "\n",
    "    # predict y_hat (target_hat) <- 良くないかも、with torch_nograd() と model.eval()\n",
    "    mlp.eval()\n",
    "    with torch.no_grad():\n",
    "#         target_test = target_test_dataset[num_window]\n",
    "#         feature_test = feature_test_dataset[num_window]\n",
    "        y_hat_umlp.append(mlp(feature_test_std).squeeze().detach().numpy())\n",
    "    #                 print(feature_test)\n",
    "    #                 print(target_test)\n",
    "    #                 print(y_hat_umlp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([92.80571 , 38.798252, 35.09224 , 40.690945, 27.085918, 31.466843,\n",
       "       38.31025 , 33.972435, 38.784184, 38.849777, 38.41575 , 43.80153 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_hat_umlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.879345506605262"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(y.iloc[-12:].values - np.array(y_hat_umlp)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.879346"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE(np.array(target_test_dataset), np.array(y_hat_umlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.300287"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAPE(np.array(target_test_dataset), np.array(y_hat_umlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25833967"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(np.array(target_test_dataset), np.array(y_hat_umlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "standardiseすると、必ずしも制度は向上しないけど、そのままだと全然収束しないようなデータだとうまく収束するときもある。\n",
    "\n",
    "\"eBASE\"は2013年度Q1に株式数が桁違いに増えている。スケールすると収束しやすくなってるかも (WOWOW, ＷＤＢホールディングスも同様)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* testを含めたスケーリングはしてはいけない(testは予測時に持っていないから。)\n",
    "* trainでスケーリング(standardize)したスケール(mean と std)でfeature_testをスケーリング。\n",
    "* train test split した後にscaling\n",
    "\n",
    "https://datascience.stackexchange.com/questions/38395/standardscaler-before-and-after-splitting-data\n",
    "\n",
    "https://stackoverflow.com/questions/63037248/is-it-correct-to-use-a-single-standardscaler-before-splitting-data\n",
    "\n",
    "* targetのスケーリングはしても意味ない\n",
    "\n",
    "https://stackoverflow.com/questions/56596653/inverse-scale-of-predicted-data-in-keras\n",
    "\n",
    "* training mode and evaluation mode\n",
    "\n",
    "https://stackoverflow.com/questions/60018578/what-does-model-eval-do-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([1, 2, 3, 4, 5])\n",
    "y_test = np.array([6])\n",
    "\n",
    "x_train = np.array([\n",
    "    [30, 4, 43],\n",
    "    [30, 5, 23],\n",
    "    [30, 2, 53],\n",
    "    [30, 3, 13],\n",
    "    [30, 1, 33],\n",
    "])\n",
    "\n",
    "x_test = np.array([\n",
    "    [60, 3, 1],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.41421356]\n",
      " [-0.70710678]\n",
      " [ 0.        ]\n",
      " [ 0.70710678]\n",
      " [ 1.41421356]]\n",
      "[[2.12132034]]\n"
     ]
    }
   ],
   "source": [
    "y_train_scaler = StandardScaler().fit(y_train.reshape(-1, 1))\n",
    "y_train_std = y_train_scaler.transform(y_train.reshape(-1, 1))\n",
    "y_test_std = y_train_scaler.transform(y_test.reshape(-1, 1))\n",
    "print(y_train_std)\n",
    "print(y_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.70710678  0.70710678]\n",
      " [ 0.          1.41421356 -0.70710678]\n",
      " [ 0.         -0.70710678  1.41421356]\n",
      " [ 0.          0.         -1.41421356]\n",
      " [ 0.         -1.41421356  0.        ]]\n",
      "[[30.         0.        -2.2627417]]\n"
     ]
    }
   ],
   "source": [
    "x_train_scaler = StandardScaler().fit(x_train)\n",
    "x_train_std = x_train_scaler.transform(x_train)\n",
    "x_test_std = x_train_scaler.transform(x_test)\n",
    "print(x_train_std)\n",
    "print(x_test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize \n",
    "target_train_scaler = StandardScaler(with_mean=False, with_std=False).fit(target_train.reshape(-1, 1))\n",
    "target_train_std = torch.tensor(target_train_scaler.transform(target_train.reshape(-1, 1)).reshape(target_train.shape), dtype=dtype)\n",
    "target_test_std = torch.tensor(target_train_scaler.transform(target_test.reshape(-1, 1)).reshape(target_test.shape), dtype=dtype)\n",
    "\n",
    "feature_train_scaler = StandardScaler().fit(feature_train)\n",
    "feature_train_std = torch.tensor(feature_train_scaler.transform(feature_train).reshape(feature_train.shape), dtype=dtype)\n",
    "feature_test_std = torch.tensor(feature_train_scaler.transform(feature_test.reshape(1, -1)).reshape(feature_test.shape), dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 3.5900e+00,  5.2500e+00,  6.6600e+00,  3.5600e+00,  7.4600e+00,\n",
       "          6.0900e+00,  4.5700e+00,  6.1000e+00,  1.4840e+01, -6.4877e-02,\n",
       "          3.0000e+00,  8.8700e+00,  2.7300e+00,  1.1614e+01,  4.1860e+01,\n",
       "          8.0890e+01,  2.7950e+01,  8.6670e+01,  7.6900e+01,  9.9700e+01,\n",
       "          6.7500e+01,  1.2117e+02,  9.0920e+01,  1.1988e+02,  6.9340e+01,\n",
       "          1.0542e+02,  9.0440e+01,  1.0624e+02,  9.9220e+01,  1.1685e+02,\n",
       "          1.0176e+02,  1.1393e+02,  7.9900e+01,  1.4523e+02,  1.2068e+02,\n",
       "          1.5335e+02]),\n",
       " tensor([ 3.5900e+00,  5.2500e+00,  6.6600e+00,  3.5600e+00,  7.4600e+00,\n",
       "          6.0900e+00,  4.5700e+00,  6.1000e+00,  1.4840e+01, -6.4877e-02,\n",
       "          3.0000e+00,  8.8700e+00,  2.7300e+00,  1.1614e+01,  4.1860e+01,\n",
       "          8.0890e+01,  2.7950e+01,  8.6670e+01,  7.6900e+01,  9.9700e+01,\n",
       "          6.7500e+01,  1.2117e+02,  9.0920e+01,  1.1988e+02,  6.9340e+01,\n",
       "          1.0542e+02,  9.0440e+01,  1.0624e+02,  9.9220e+01,  1.1685e+02,\n",
       "          1.0176e+02,  1.1393e+02,  7.9900e+01,  1.4523e+02,  1.2068e+02,\n",
       "          1.5335e+02]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_train, target_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(61.4034), tensor(61.4034))"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test, target_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 6.2200e+00,  7.6100e+00,  4.0600e+00, -1.2000e+00],\n",
       "         [ 3.5900e+00,  6.2200e+00,  7.6100e+00,  4.0600e+00],\n",
       "         [ 5.2500e+00,  3.5900e+00,  6.2200e+00,  7.6100e+00],\n",
       "         [ 6.6600e+00,  5.2500e+00,  3.5900e+00,  6.2200e+00],\n",
       "         [ 3.5600e+00,  6.6600e+00,  5.2500e+00,  3.5900e+00],\n",
       "         [ 7.4600e+00,  3.5600e+00,  6.6600e+00,  5.2500e+00],\n",
       "         [ 6.0900e+00,  7.4600e+00,  3.5600e+00,  6.6600e+00],\n",
       "         [ 4.5700e+00,  6.0900e+00,  7.4600e+00,  3.5600e+00],\n",
       "         [ 6.1000e+00,  4.5700e+00,  6.0900e+00,  7.4600e+00],\n",
       "         [ 1.4840e+01,  6.1000e+00,  4.5700e+00,  6.0900e+00],\n",
       "         [-6.4877e-02,  1.4840e+01,  6.1000e+00,  4.5700e+00],\n",
       "         [ 3.0000e+00, -6.4877e-02,  1.4840e+01,  6.1000e+00],\n",
       "         [ 8.8700e+00,  3.0000e+00, -6.4877e-02,  1.4840e+01],\n",
       "         [ 2.7300e+00,  8.8700e+00,  3.0000e+00, -6.4877e-02],\n",
       "         [ 1.1614e+01,  2.7300e+00,  8.8700e+00,  3.0000e+00],\n",
       "         [ 4.1860e+01,  1.1614e+01,  2.7300e+00,  8.8700e+00],\n",
       "         [ 8.0890e+01,  4.1860e+01,  1.1614e+01,  2.7300e+00],\n",
       "         [ 2.7950e+01,  8.0890e+01,  4.1860e+01,  1.1614e+01],\n",
       "         [ 8.6670e+01,  2.7950e+01,  8.0890e+01,  4.1860e+01],\n",
       "         [ 7.6900e+01,  8.6670e+01,  2.7950e+01,  8.0890e+01],\n",
       "         [ 9.9700e+01,  7.6900e+01,  8.6670e+01,  2.7950e+01],\n",
       "         [ 6.7500e+01,  9.9700e+01,  7.6900e+01,  8.6670e+01],\n",
       "         [ 1.2117e+02,  6.7500e+01,  9.9700e+01,  7.6900e+01],\n",
       "         [ 9.0920e+01,  1.2117e+02,  6.7500e+01,  9.9700e+01],\n",
       "         [ 1.1988e+02,  9.0920e+01,  1.2117e+02,  6.7500e+01],\n",
       "         [ 6.9340e+01,  1.1988e+02,  9.0920e+01,  1.2117e+02],\n",
       "         [ 1.0542e+02,  6.9340e+01,  1.1988e+02,  9.0920e+01],\n",
       "         [ 9.0440e+01,  1.0542e+02,  6.9340e+01,  1.1988e+02],\n",
       "         [ 1.0624e+02,  9.0440e+01,  1.0542e+02,  6.9340e+01],\n",
       "         [ 9.9220e+01,  1.0624e+02,  9.0440e+01,  1.0542e+02],\n",
       "         [ 1.1685e+02,  9.9220e+01,  1.0624e+02,  9.0440e+01],\n",
       "         [ 1.0176e+02,  1.1685e+02,  9.9220e+01,  1.0624e+02],\n",
       "         [ 1.1393e+02,  1.0176e+02,  1.1685e+02,  9.9220e+01],\n",
       "         [ 7.9900e+01,  1.1393e+02,  1.0176e+02,  1.1685e+02],\n",
       "         [ 1.4523e+02,  7.9900e+01,  1.1393e+02,  1.0176e+02],\n",
       "         [ 1.2068e+02,  1.4523e+02,  7.9900e+01,  1.1393e+02]]),\n",
       " tensor([[-1.0629, -0.9799, -1.0134, -1.0687],\n",
       "         [-1.1180, -1.0094, -0.9350, -0.9538],\n",
       "         [-1.0832, -1.0651, -0.9657, -0.8762],\n",
       "         [-1.0537, -1.0299, -1.0238, -0.9066],\n",
       "         [-1.1186, -1.0000, -0.9871, -0.9641],\n",
       "         [-1.0370, -1.0657, -0.9560, -0.9278],\n",
       "         [-1.0657, -0.9831, -1.0244, -0.8970],\n",
       "         [-1.0975, -1.0121, -0.9383, -0.9647],\n",
       "         [-1.0655, -1.0443, -0.9686, -0.8795],\n",
       "         [-0.8826, -1.0119, -1.0021, -0.9094],\n",
       "         [-1.1944, -0.8268, -0.9684, -0.9426],\n",
       "         [-1.1303, -1.1425, -0.7754, -0.9092],\n",
       "         [-1.0075, -1.0776, -1.1045, -0.7182],\n",
       "         [-1.1360, -0.9533, -1.0368, -1.0439],\n",
       "         [-0.9501, -1.0833, -0.9072, -0.9769],\n",
       "         [-0.3173, -0.8951, -1.0428, -0.8487],\n",
       "         [ 0.4993, -0.2546, -0.8466, -0.9828],\n",
       "         [-0.6083,  0.5719, -0.1789, -0.7887],\n",
       "         [ 0.6202, -0.5492,  0.6827, -0.1278],\n",
       "         [ 0.4158,  0.6943, -0.4860,  0.7250],\n",
       "         [ 0.8928,  0.4874,  0.8103, -0.4318],\n",
       "         [ 0.2191,  0.9702,  0.5947,  0.8513],\n",
       "         [ 1.3420,  0.2883,  1.0980,  0.6378],\n",
       "         [ 0.7091,  1.4248,  0.3871,  1.1360],\n",
       "         [ 1.3150,  0.7843,  1.5720,  0.4324],\n",
       "         [ 0.2576,  1.3975,  0.9042,  1.6051],\n",
       "         [ 1.0125,  0.3273,  1.5435,  0.9441],\n",
       "         [ 0.6991,  1.0913,  0.4278,  1.5769],\n",
       "         [ 1.0296,  0.7741,  1.2243,  0.4726],\n",
       "         [ 0.8828,  1.1087,  0.8936,  1.2609],\n",
       "         [ 1.2516,  0.9600,  1.2424,  0.9336],\n",
       "         [ 0.9359,  1.3334,  1.0874,  1.2789],\n",
       "         [ 1.1905,  1.0138,  1.4766,  1.1255],\n",
       "         [ 0.4786,  1.2715,  1.1435,  1.5107],\n",
       "         [ 1.8454,  0.5509,  1.4122,  1.1810],\n",
       "         [ 1.3317,  1.9343,  0.6609,  1.4469]]))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_train, feature_train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([153.3500, 120.6800, 145.2300,  79.9000]),\n",
       " tensor([2.0153, 1.4145, 2.1031, 0.7033]))"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_test, feature_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([153.3500, 120.6800, 145.2300,  79.9000])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(feature_train_scaler.inverse_transform(feature_test_std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
